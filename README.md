# 297-test-3
Creating an SVM classifier to predict the biodegradability of molecules

### EDA 
Before EDA, we instantiated a list of the column names from the attached txt file then used a label encoder to turn the text target into a binary classification. We then printed the number of blanks or nans from each feature (we used a graph but there are so many it is easier to simply print them out). We used the sklearn SimpleImputer to attempt filling na values with the mean value then with the median value but found that both lowered the accuracy of our model and generally seemed like a poor design choice. Therefore, we decided to go with dropping the na values especially when the list shows that there are relatively few for the size of the dataset. We then attempted to run a correlation heatmap which displays but needs to be viewed on a large screen or with the zoom function to discern anything because of the number of features. We noticed that the majority of features had almost zero correlation with the target variable letting us know that feature selection was a necessity. 

### Feature Scaling/Selection/Extraction
For scalers, we attempted the full gamit of applicable options but found that the standard scaler worked the best for both pca and lda as well as most of the permutations of hyperparameters. Therefore, we decided to stick with the standard scaler. For pca, we ran a for loop with different values of components for the model along with a grid search of potential parameters to find the best permutation. We also utilized univariate feature selection but this significantly worsened the performance of the model. Finally, we also attempted lda but found that it performed less well than the best permutation of hyperparameters and components from the pca, we also read online that its possible to run lda after pca so we attempted that as well but found it decreased the overall accuracy of our model significantly so removed it. After running the grid search and finding the best permuations of hyperparameters we continued to tweak the values slightly to better optimize performance. For example we found c=100 worked best from the selections we outlined in the grid search but upon finer tweaking found that c=500 actually performs better than 100 and the same with our final gamma value. 

### Model Evaluation
To evaluate the accuracy of the model, we did a simply accuracy score for a general view of performance as well as runnning a classification report to see the precision, recall, and f1 for each target value as well as for the overall model of test data. We also ran a confusion matrix to visually demonstrate exact values of classification for each label. From this evaluation, we found that the model performed alright but did significantly better classifying NRB (label 0) than RB (label 1). 
We found when increasing the size of the training set, the model performed better especially for the RB label classification which made us believe that one of the issues with our model hurting accuracy is that there are not enough examples to train on. There are nearly twice as many (699 v 356) NRBs as RBs which would help back up this theory that the model struggled classifying that label since there are less examples of it. Generally, we think that the cross validation scores show that the model is decently generalizable with a certain degree of confidence but that is not extremely helpful considering the model is only 87% accurate. Given more data to train the svm along with a decently sized test/validation set would probably help increase but the generalizability of the model as well as its overall accuracy, specifically in classifying the RB label

